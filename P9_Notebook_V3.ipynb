{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751d297b47f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0. RECONFIGURATION SPARK POUR LA PCA\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stopper la session actuelle si elle existe\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"✓ Session Spark existante arrêtée.\")\n",
    "except:\n",
    "    print(\"ℹ️ Aucune session Spark à arrêter.\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"P9-ImageProcessing-PCA\") \\\n",
    "    .config(\"spark.driver.memory\", \"30g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"20g\") \\\n",
    "    .config(\"spark.executor.memory\", \"20g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"\\n✓ Configuration Spark optimisée pour la PCA :\")\n",
    "print(f\"  - Driver Memory      : {spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"  - Max Result Size    : {spark.conf.get('spark.driver.maxResultSize')}\")\n",
    "print(f\"  - Executor Memory    : {spark.conf.get('spark.executor.memory')}\")\n",
    "print(f\"  - Shuffle Partitions : {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9b6083e83a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0.5 ALERTE COÛTS ET INITIALISATION\n",
    "# =============================================================================\n",
    "\n",
    "# ALERTE COÛTS : Instance EMR active\n",
    "# Coût estimé : ~2.70€/heure pour ce cluster (3x i3.2xlarge).\n",
    "import datetime\n",
    "print(f\"Session démarrée à : {datetime.datetime.now()}\")\n",
    "print(\"N'oubliez pas d'arrêter le cluster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619ae741ef7116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. SETUP ET CONFIGURATION DE L'ENVIRONNEMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Affichage des informations sur la session Spark\n",
    "print(\"\\nInformations sur la session Spark :\")\n",
    "spark\n",
    "\n",
    "try:\n",
    "    region = spark.conf.get('spark.hadoop.fs.s3a.endpoint.region')\n",
    "    if region not in ['eu-west-1', 'eu-west-2', 'eu-west-3', 'eu-central-1', 'eu-north-1']:\n",
    "        raise ValueError(f\"❌ ERREUR RGPD : Région {region} hors UE !\")\n",
    "    print(f\"\\n✓ Région S3 conforme RGPD : {region}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Vérification RGPD échouée : {e}\")\n",
    "\n",
    "# Calcul du nombre de partitions idéal\n",
    "num_partitions = spark.sparkContext.defaultParallelism * 2\n",
    "print(f\"✓ Nombre de coeurs disponibles : {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"✓ Nombre de partitions utilisé : {num_partitions} (justifié par le nombre de coeurs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cb744ee504378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. IMPORT DES LIBRAIRIES\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, element_at, split\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "print(\"\\n✓ Toutes les librairies ont été importées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0aaf40ee2f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. CHARGEMENT ET PRÉPARATION DES DONNÉES\n",
    "# =============================================================================\n",
    "\n",
    "PATH_Data = \"s3a://projet9-aws/Test_Sample\"\n",
    "#PATH_Data = \"s3a://projet9-tmoahs/Training\"\n",
    "PATH_Result = \"s3a://projet9-aws/Test_Results\"\n",
    "\n",
    "print(f\"\\nChargement des images depuis : {PATH_Data}\")\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)\n",
    "\n",
    "images = images.repartition(num_partitions)\n",
    "\n",
    "print(f\"✓ {images.count()} images chargées et réparties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6623f2125b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. EXTRACTION DE FEATURES AVEC MOBILENETV2 (VERSION CORRIGÉE)\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "model_intermediaire = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "broadcast_weights = spark.sparkContext.broadcast(model_intermediaire.get_weights())\n",
    "print(\"\\n✓ Poids du modèle broadcastés.\")\n",
    "\n",
    "# Retourner un ArrayType au lieu de VectorUDT\n",
    "def featurize_series(model, content_series: pd.Series) -> pd.Series:\n",
    "    def featurize_udf(content):\n",
    "        local_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "        intermediate_model = Model(inputs=local_model.input, outputs=local_model.layers[-2].output)\n",
    "        intermediate_model.set_weights(broadcast_weights.value)\n",
    "\n",
    "        img = Image.open(io.BytesIO(content)).resize((224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        expanded_img = np.expand_dims(img_array, axis=0)\n",
    "        preprocessed_img = preprocess_input(expanded_img)\n",
    "\n",
    "        features = intermediate_model.predict(preprocessed_img)\n",
    "        flattened_features = features.flatten()\n",
    "\n",
    "        # Retourner une liste Python, pas un Vector\n",
    "        return flattened_features.tolist()\n",
    "\n",
    "    return content_series.apply(featurize_udf)\n",
    "\n",
    "# Déclarer le returnType comme ArrayType(FloatType())\n",
    "featurize_udf = pandas_udf(\n",
    "    lambda content_series: featurize_series(model, content_series),\n",
    "    returnType=ArrayType(FloatType())\n",
    ")\n",
    "\n",
    "print(\"Début de l'extraction des features...\")\n",
    "features_df = images.select(\n",
    "    col(\"path\"),\n",
    "    element_at(split(col(\"path\"), \"/\"), -2).alias(\"label\"),\n",
    "    featurize_udf(col(\"content\")).alias(\"features\")\n",
    ")\n",
    "print(\"✓ Extraction terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669037ab59bd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. RÉDUCTION DE DIMENSION (PCA) - VERSION CORRIGÉE\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# ÉTAPE 1 : Convertir les listes en Vectors\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "features_df_vectors = features_df.withColumn(\"featuresVector\", list_to_vector_udf(col(\"features\")))\n",
    "\n",
    "# ÉTAPE 2 : Persister pour optimiser\n",
    "features_df_vectors = features_df_vectors.persist()\n",
    "print(\"\\n✓ DataFrame de features mis en cache.\")\n",
    "\n",
    "# Vérification\n",
    "print(\"✓ Vérification du schéma :\")\n",
    "features_df_vectors.select(\"featuresVector\").printSchema()\n",
    "\n",
    "# ÉTAPE 3 : Appliquer la PCA sur la colonne Vector\n",
    "print(\"\\nDébut de l'entraînement de la PCA (k=100)...\")\n",
    "pca = PCA(k=100, inputCol=\"featuresVector\", outputCol=\"pcaFeatures\")\n",
    "model_pca = pca.fit(features_df_vectors)\n",
    "df_pca = model_pca.transform(features_df_vectors)\n",
    "print(\"✓ PCA terminée.\")\n",
    "\n",
    "# Aperçu du résultat\n",
    "print(f\"\\n✓ Nombre d'images traitées : {df_pca.count()}\")\n",
    "df_pca.select(\"path\", \"label\", \"pcaFeatures\").show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38808a5acaeca17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. SAUVEGARDE DES RÉSULTATS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nSauvegarde des résultats dans : {PATH_Result}\")\n",
    "df_pca.write.mode(\"overwrite\").parquet(PATH_Result)\n",
    "print(\"✓ Sauvegarde terminée.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 7. VALIDATION DES RÉSULTATS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATION DES DONNÉES TRAITÉES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 7.1 Vérification du nombre total d'images\n",
    "print(f\"\\n1. Nombre d'images:\")\n",
    "initial_count = images.count()\n",
    "final_count = df_pca.count()\n",
    "print(f\"   - Images chargées initialement : {initial_count}\")\n",
    "print(f\"   - Images après traitement PCA : {final_count}\")\n",
    "if initial_count == final_count:\n",
    "    print(\"   ✓ Aucune perte de données\")\n",
    "else:\n",
    "    print(f\"   ✗ ATTENTION : {initial_count - final_count} images perdues !\")\n",
    "\n",
    "# 7.2 Vérification des dimensions\n",
    "print(f\"\\n2. Dimensions des features:\")\n",
    "sample_features = df_pca.select(\"featuresVector\", \"pcaFeatures\").first()\n",
    "original_dim = len(sample_features['featuresVector'])\n",
    "pca_dim = len(sample_features['pcaFeatures'])\n",
    "print(f\"   - Dimension originale (MobileNetV2) : {original_dim}\")\n",
    "print(f\"   - Dimension après PCA : {pca_dim}\")\n",
    "print(f\"   - Réduction : {((1 - pca_dim/original_dim) * 100):.1f}%\")\n",
    "\n",
    "# 7.3 Vérification de l'intégrité (pas de NaN/Null)\n",
    "print(f\"\\n3. Intégrité des données:\")\n",
    "null_features = df_pca.filter(col(\"featuresVector\").isNull()).count()\n",
    "null_pca = df_pca.filter(col(\"pcaFeatures\").isNull()).count()\n",
    "print(f\"   - Features NULL : {null_features}\")\n",
    "print(f\"   - PCA Features NULL : {null_pca}\")\n",
    "if null_features == 0 and null_pca == 0:\n",
    "    print(\"   ✓ Aucune valeur manquante\")\n",
    "else:\n",
    "    print(\"   ✗ ATTENTION : Valeurs manquantes détectées !\")\n",
    "\n",
    "# 7.4 Variance expliquée par la PCA\n",
    "print(f\"\\n4. Variance expliquée par la PCA:\")\n",
    "explained_variance = model_pca.explainedVariance\n",
    "total_variance = sum(explained_variance)\n",
    "print(f\"   ✓ Variance totale expliquée par {pca_dim} composantes : {total_variance*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATION TERMINÉE\")\n",
    "print(\"=\" * 80 + \"\\n\")"
   ],
   "id": "3ed0a25d78c8ed33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07566361ad623c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. RETOUR CRITIQUE\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "## RETOUR CRITIQUE SUR L'ARCHITECTURE BIG DATA\n",
    "\n",
    "### Avantages\n",
    "- **Scalabilité Horizontale** : L'architecture EMR a prouvé sa capacité à traiter un volume de données ingérable sur une seule machine, simplement en ajoutant des nœuds.\n",
    "- **Performance** : En utilisant des instances optimisées pour le stockage (`i3`) et en parallélisant les calculs avec Spark, nous avons atteint des temps de traitement performants pour un coût maîtrisé.\n",
    "- **Écosystème Intégré** : L'intégration native d'EMR avec S3 simplifie grandement le pipeline de données, du stockage brut au stockage des résultats.\n",
    "\n",
    "### Inconvénients\n",
    "- **Coût de Démarrage (\"Cold Start\")** : Le temps de provisionnement du cluster (~10-15 min) représente un coût fixe et un délai incompressibles, rendant la solution peu rentable pour des traitements très courts ou des volumes de données faibles (< 50 Go).\n",
    "- **Complexité Opérationnelle** : La configuration, l'optimisation (gestion de la mémoire, des partitions) et le débogage d'un cluster EMR demandent des compétences spécifiques, comme l'a démontré ce projet.\n",
    "\n",
    "### Recommandations\n",
    "- **Utilisation Ciblée** : Réserver l'usage d'EMR pour les traitements dépassant un seuil de rentabilité (estimé à > 50-100 Go de données).\n",
    "- **Automatisation** : Pour une utilisation en production, automatiser le cycle de vie du cluster (création, exécution, arrêt) via des services comme AWS Step Functions ou Lambda pour fiabiliser le processus et maîtriser les coûts.\n",
    "- **Spot Instances** : Utiliser systématiquement des instances Spot pour les nœuds de travail afin de réduire les coûts d'EC2 jusqu'à 70%.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0228cdbfb7185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9. RECOMMANDATIONS POUR LA PRODUCTION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMANDATIONS POUR LE PASSAGE EN PRODUCTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. INFRASTRUCTURE\n",
    "   - Utiliser EMR avec Auto Scaling (min 2, max 10 nodes) pour s'adapter à la charge.\n",
    "   - Privilégier les Spot instances pour réduire les coûts de 70%.\n",
    "   - Configurer un timeout automatique si inactivité > 1h.\n",
    "\n",
    "2. MONITORING\n",
    "   - Activer CloudWatch pour suivre les métriques Spark et l'utilisation des ressources.\n",
    "   - Mettre en place des alertes si la durée du traitement dépasse un seuil.\n",
    "\n",
    "3. DONNÉES\n",
    "   - Partitionner les données sur S3 par date (ex: `year=2025/month=10/`) pour éviter de scanner tout le dataset.\n",
    "   - Utiliser une politique de cycle de vie S3 pour archiver les vieilles données.\n",
    "\n",
    "4. SÉCURITÉ & RGPD\n",
    "   - Activer le chiffrement S3 au repos (SSE-S3).\n",
    "   - Lancer le cluster dans un VPC privé sans accès public direct.\n",
    "   - Activer les logs d'audit avec AWS CloudTrail.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8025196957074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. NETTOYAGE FINAL\n",
    "# =============================================================================\n",
    "\n",
    "# Libération des ressources mises en cache\n",
    "features_df_vectors.unpersist()\n",
    "broadcast_weights.unpersist()\n",
    "print(\"\\n✓ Cache mémoire libéré.\")\n",
    "\n",
    "# Arrêt de la session Spark\n",
    "spark.stop()\n",
    "print(\"✓ Session Spark terminée. Vous pouvez maintenant arrêter le cluster EMR.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
